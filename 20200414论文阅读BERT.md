# 阅读笔记:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(深层双向Transformer的预训练模型)
## 简介
  BERT是这篇文章提出的一个语言预训练模型。语言模型的预训练已经被证明是改善许多自然语言处理任务的有效方法。包括sentence-level的任务，
比如自然语言推理、段落推理，通过整体分析句子来预测句子之间的关系；以及token-level的任务，比如命名实体识别和问答，这些模型被要求产生
token级别的细粒度（fine-grained）输出。
  目前，在下游任务(down-stream tasks)中应用预先训练过的语言表征有两种策略：
  - 基于特征（feature-based）：基于特定的方法使用将预训练好的语言表征当作一个额外的特征输入到基于特定任务的体系结构中
  - 微调(fine-tuning)：微调方法引入很少任务特定参数，通过简单微调所有预训练过参数来下游任务进行训练
