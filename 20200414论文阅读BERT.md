# 阅读笔记:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(深层双向Transformer的预训练模型)
## 简介
  BERT是这篇文章提出的一个语言预训练模型。语言模型的预训练已经被证明是改善许多自然语言处理任务的有效方法。包括sentence-level的任务，
比如自然语言推理、段落推理，通过整体分析句子来预测句子之间的关系；以及token-level的任务，比如命名实体识别和问答，这些模型被要求产生
token级别的细粒度（fine-grained）输出。
  目前，在下游任务(down-stream tasks)中应用预先训练过的语言表征有两种策略：
  - 基于特征（feature-based）：基于特定的方法使用将预训练好的语言表征当作一个额外的特征输入到基于特定任务的体系结构中
  - 微调(fine-tuning)：微调方法引入很少任务特定参数，通过简单微调所有预训练过参数来下游任务进行训练
  目前的局限：标准语言模型是单向的，这就限制了在预训练过程中可选的体系结构。比如OpenAI GPT中使用了一个left-to-right的结构，Transformer中self-attention layers中，每个token都只能根据关注在它前面的token。这种限制导致sentence-level任务是次优的，并且对使用fine-tuning方法来实现token-level任务是非常有害的，例如question answering中，前后文的信息非常重要。ELMo使用分别训练left-to-right和right-to-left语言模型的浅级联表征。
  基于此，作者就提出BERT来进行改进。BERT通过使用“遮蔽语言模型”（masked language model, MLM）预训练目标来缓解先前提到的单向性约束。遮蔽语言模型随机遮蔽输入的一些token,目标是去预测原文中被遮蔽后的词汇ID。不同于之前的单向约束，MLM能使语言表征融合左右文本信息。除了遮蔽语言模型，论文还训练了“下一个句子预测”任务，联合地对文本对的表征进行了学习。

## 相关工作
### 基于特征的非监督方法
  在方面的工作有：word embedding，sentence embedding，paragragh embedding  
  ELMo分别使用了left-to-right和right-to-left的语言模型来提取上下文敏感(context-sensitive)的特征。然后concatenation这两个特征来表示token的上下文表征（word embedding)。
[img](https://img-blog.csdnimg.cn/20190619133650508.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NzZG5fbWlzbGk=,size_16,color_FFFFFF,t_70)

### 基于微调的非监督方法
