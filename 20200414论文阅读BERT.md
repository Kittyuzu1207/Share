# 阅读笔记:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(深层双向Transformer的预训练模型)
## 简介
BERT是这篇文章提出的一个语言预训练模型。语言模型的预训练已经被证明是改善许多自然语言处理任务的有效方法。包括sentence-level的任务，
比如自然语言推理、段落推理，通过整体分析句子来预测句子之间的关系；以及token-level的任务，比如命名实体识别和问答，这些模型被要求产生
token级别的细粒度（fine-grained）输出。
