# 阅读笔记:BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding(深层双向Transformer的预训练模型)
## 简介
BERT是这篇文章提出的一个语言预训练模型。语言模型的预训练已经被证明是改善许多自然语言处理任务的有效方法。包括sentence-level的任务，
比如自然语言推理、段落推理，通过整体分析句子来预测句子之间的关系；以及token-level的任务，比如命名实体识别和问答，这些模型被要求产生
token级别的细粒度（fine-grained）输出。
***
  目前，在下游任务(down-stream tasks)中应用预先训练过的语言表征有两种策略：
  - 基于特征（feature-based）：基于特定的方法使用将预训练好的语言表征当作一个额外的特征输入到基于特定任务的体系结构中
  - 微调(fine-tuning)：微调方法引入很少任务特定参数，通过简单微调所有预训练过参数来下游任务进行训练
***
目前的局限：标准语言模型是单向的，这就限制了在预训练过程中可选的体系结构。比如OpenAI GPT中使用了一个left-to-right的结构，Transformer中self-attention layers中，每个token都只能根据关注在它前面的token。这种限制导致sentence-level任务是次优的，并且对使用fine-tuning方法来实现token-level任务是非常有害的，例如question answering中，前后文的信息非常重要。ELMo使用分别训练left-to-right和right-to-left语言模型的浅级联表征。
基于此，作者就提出BERT来进行改进。BERT通过使用“遮蔽语言模型”（masked language model, MLM）预训练目标来缓解先前提到的单向性约束。遮蔽语言模型随机遮蔽输入的一些token,目标是去预测原文中被遮蔽后的词汇ID。不同于之前的单向约束，MLM能使语言表征融合左右文本信息。除了遮蔽语言模型，论文还训练了“下一个句子预测”任务，联合地对文本对的表征进行了学习。

## 相关工作
### 基于特征的非监督方法
在方面的工作有：word embedding，sentence embedding，paragragh embedding  
ELMo分别使用了left-to-right和right-to-left的语言模型来提取上下文敏感(context-sensitive)的特征。然后concatenation这两个特征来表示token的上下文表征（word embedding)。
![img](https://img-blog.csdnimg.cn/20190619133650508.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NzZG5fbWlzbGk=,size_16,color_FFFFFF,t_70)
### 基于微调的非监督方法
句子或文档编码器通过没有标记的文本上进行预训练得到上下文的词表征，然后在下游任务中进行监督微调。这中方法的优点是只有很少的参数需要需要从头开始学习。
### 从监督数据中进行迁移学习
从大量数据中进行训练然后迁移到特定任务中进行监督学习，在自然语言处理任务中，如自然语言推理和机器翻译，以及计算机视觉任务中都表面了其有优越性。

## BERT
BERT包含pre-training和fine-tuning两个模块/过程。  
在预训练(pre-training)期间，在不同的预训练任务上使用未标记的数据对该模型进行的训练（同一个模型）。  
在微调（fine-tuning）期间，先用预训练的参数初始化BERT模型，并且使用来自下游任务的标记数据对所有参数进行微调。每个下游任务都有单独的微调模型，即使它们是用相同的预训练参数初始化的。
![img](https://img-blog.csdnimg.cn/20190619133724708.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NzZG5fbWlzbGk=,size_16,color_FFFFFF,t_70)
BERT的一个显著特点是其同一个网络结构可以跨越不同的任务，在预训练的网络结构和最终的下游任务结构之间只有很小的不同（最后多一个输出层）  
### Model Architecture 模型结构
BERT的模型结构是一个多层的双向Transformer编码器,Transformer几乎直接是基于Vaswani et al. (2017)的原始实现。
- L 表示网络的层数，即Transformer block的个数  
- H 隐藏层维度  
- A self-attention head的数量  
在论文所有的cases里都将feed-forward/filter的大小设置为4H，即3072对应于H=768，4096对应于H=1024。论文主要展示了两种模型大小的结果。一是BERT BASE (L=12,H=768,A=12,参数量：110M)，二是BERT LARGER (L=24,H=1024,A=16,参数量：340M)  
ps:在这篇论文中，将双向Transformer称为Transformer编码器（encoder），将OpenAI GPT中的单向Transformer称为Transformer解码器（decoder），因为它只能关注前面（左侧）的上下文。

### Input/Output Representations
为了使BERT可以处理各种下游任务，在一个token序列中，它的输入表征能够同时明确的表达一个单个句子和一对句子。在整个工作中，“句子”可以是任意的连续文本，而不是实际的语料，“序列”指的是BERT的输入token序列，可以是单句或两个句子打包在一起。论文使用了具有30,000词汇大小的WordPiece embedding。（ (Wu et al.,2016)）。
***
句子头部使用了一个特殊标记[CLS]，与这个标记对应的最后隐藏状态可以在分类任务中用来作为这个输入句子的表征。当句子对（pair）融合为一个句子时，论文用两种方式来区分它们：一是使用一个特殊的token[SEP]，二是在每个token中添加一个学习过的embedding来指示它是属于句子A还是句子B。如图1所示，输入的embedding表示为E，最后的隐藏层向量中，[CLS]是指句子的表征C∈R^H，第i个向量是指第i个输入token的输出表征Ti∈R^H。  
对于给定的token，它的输入表征是通过对相应的token、segment和position embedding的求和来构造的。在图2中可以看到这个结构的可视化。  
![pic2](https://img-blog.csdnimg.cn/20190619133806557.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NzZG5fbWlzbGk=,size_16,color_FFFFFF,t_70)  

### 3.1 Pre-training BERT
不同于传统的left-to-right 或right-to-left 预训练，BERT预训练是一个多任务模型，它的任务是由两个自监督任务组成，即MLM和NSP。  
**TASK 1: Masked LM (MLM)遮蔽模型**   
直观地讲，深度双向模型比left-to-right模型或left-to-right模型和right-to-left模型的串联更强大。 *但是标准的条件语言模型只能只能left-to-right或right-to-left进行单向训练，因为双向条件训练可以使每个词间接地“看到自己”，模型可以在多层上下文中轻松地预测目标词。*  
![img](https://img-blog.csdnimg.cn/2019061913382754.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NzZG5fbWlzbGk=,size_16,color_FFFFFF,t_70)
***
为了训练深度双向表示，作者随机地遮掩一定百分比的输入token，然后预测这些被遮掩的token，这个过程称为“masked LM”(MLM)，可以理解为完型填空。在这种情况下，与maked token对应的最终隐藏向量被输入到对应词汇表的输出Softmax中。作者在实验中遮掩了输入序列15%的token。与去噪自动编码器不同，BERT只预测maksed token，而不是重构整个输入。  
虽然这可以获得一个双向的预训练模型，但缺点是在预训练模型和下游任务微调之间造成了不匹配，因为[MASK] token并不会出现在微调中，为了减轻这个问题，BERT并不总是用[MAKS]去遮掩需要被遮掩的token，训练数据随机选择15%的token位置来进行预测，如果第i个token被选择了，那么它有80%的概率被[MASK]替换，10%的概率被随机token替换和10%的概率不改变。使用交叉熵损失来得到预测原始token的Ti。  
**TASK 2：Next Sentence Prediction (NSP)预测下一个句子**  
许多重要的下游任务，如问答(QA)和自然语言推理(NLI)，都是基于理解两句之间的关系，而不是通过语言建模直接捕捉到的。为了训练模型能理解句子之间的关系，论文作者对下一个句子预测预训练了一个二分类任务。对于每个训练样本都包含一个句子A和一个句子B，让模型去判断B是否是A的下一句，一般B有50%的概率是A的真实下一句，对其标记为IsNext，有50%的概率是语料库中的随机句子，对其标记为NotNext。在图1的模型结构中，C（一般可用于句子分类）可以用来判断B是否是A的下一句。这种操作虽然简单，但是效果极好，在下一个句子预测中能达到97%-98%的准确率，预训练的模型对QA和NLI的提升也很大。  
***
对比：word2vec的一个精髓是引入了一个优雅的负采样任务来学习词向量（word-level representation），BERT使用句子级负采样任务学到句子表示。同时在句子表示上，BERT这里并没有像下游监督任务中的普遍做法一样，在encoding的基础上再搞个全局池化之类的，它首先在每个sequence（对于句子对任务来说是两个拼起来的句子，对于其他任务来说是一个句子）前面加了一个特殊的token[CLS]。然后让encoder对[CLS]进行深度encoding，深度encoding的最高隐层即为整个句子/句对的表示。这个做法乍一看有点费解，不过Transformer是可以无视空间和距离的把全局信息encoding进每个位置的，而[CLS]作为句子/句对的表示是直接跟分类器的输出层连接的，因此其作为梯度反传路径上的“关卡”，当然会想办法学习到分类相关的上层特征。
### Pre-training data
因为模型要学习词和上下文已经句子和上下文的关系，需要使用document-level的语料库，而不能使用sentence-level的语料库。BooksCorpus (800M words)
English Wikipedia (2,500M words)。只提取文本段落，忽略列表、表和标题

### 3.2 Fine-tuning BERT微调BERT
微调很简单，因为Transformer中的self-attention机制允许BERT通过交换适当的输入和输出，对许多下游任务进行建模，无论这些任务涉及单个文本还是文本对。
对于需要处理文本对的应用，常见的做法是先分别对文本对进行编码，然后再应用双向交叉注意力机制。但是BERT改用自注意力机制来统一了这两个阶段，因为自注意力机制对拼接的文本对编码时包括了两个句子间的双向交叉注意力。  
对于每个任务，只需将任务的输入和输出插入到BERT中，然后端到端的训练所有参数。句子A和句子B被统一指代输入的两个句子，可以是（1）段落的句子对，（2）推理中的假设-前提对，（3）问答对以及（4）针对文本分类或者序列标注的退化text-null（B为空）。在输出部分，token的表征可以输入到额外添加的输出层中进行token-level的任务，比如序列序列标注或者问答，[CLS]的表征可以表示为一个句子的表征，可以输入到输出层进行分类，例如推理或者情感分析任务。
在不同任务上的模型效果  
![img](https://img-blog.csdnimg.cn/20190619133851319.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NzZG5fbWlzbGk=,size_16,color_FFFFFF,t_70)
