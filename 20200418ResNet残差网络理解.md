# ResNet残差网络理解  
## 1 Background 提出背景  
### Forward：from shallow to deep
- 深度学习省去了人工寻找特征的步骤，不同的模型也找出不同质量的特征，特征的质量直接影响到分类结果的准确度，表达能力更强的特征也给模型带来更强的分类能力。
- 特征也可以根据复杂度和表示能力粗略的分为高中低三种种类，理论上讲越复杂的特征有越强的表征能力。一般越深的网络输出表示能力越强的特征。所以，网络的深度对于学习表达能力更强的特征至关重要。
- 深度模型中，每层的输出特征图的尺寸大都随着网络深度而变化，主要是高和宽越来越小，输出特征图的深度随着网络层数的深度而增加。高和宽的减小有助于减小计算量，而特征图深度的增加则使每层输出中可用特征数量的增多。

### Backward：the problem caused by increasing depth
- 增加深度带来的首个问题就是梯度爆炸/消散的问题，这是由于随着层数的增多，在网络中反向传播的梯度会随着连乘变得不稳定，变得特别大或者特别小。
- 为了克服梯度消散也想出了许多的解决办法，如使用BatchNorm，将激活函数换为ReLu，使用Xaiver初始化等，可以说梯度消散已经得到了很好的解决。
- 增加深度的另一个问题就是网络的degradation问题，即随着深度的增加，网络的性能会越来越差，直接体现为在训练集上的准确率会下降。**ResNet就是解决这个问题**  

## 2 Degradation of deep network 深层网络的退化
![img](https://img-blog.csdnimg.cn/2018121719305117.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzYyNDUzOA==,size_16,color_FFFFFF,t_70)  
随着网络深度的增加网络在CIFAR10-数据集上分类的训练集的错误率,可以看到如果直接堆叠卷积层，随着层数的增加，错误率有明显上升的趋势。网络衰退问题确实存在  
训练集错误率的下降说明degredation的问题并不是过拟合所造成，在作者的另一篇论文《Identity Mappings in Deep Residual Networks》中证明了degradation的产生是由于优化性能不好，这说明越深的网络反向梯度越难传导。

## 3 Deep Residual Networks 深层残差网络  
### 3.1 From 10 to 100 layers
可以设想一下，当直接对网络进行简单的堆叠到特别长，网络内部的特征在其中某一层已经达到了最佳的情况，这时候剩下层就应该不对改特征做任何改变，自动学成**恒等映射(identity mapping)** 的形式。也就是说，对一个特别深的深度网络而言，该网络的浅层形式的解空间应该是这个深度网络解空间的子集。也就是，相对于浅层网络更深的网络至少不会有更差的效果，但是因为网络degradation的问题，这并不成立。  
我们退而求其次，已知有网络degradation的情况下，不求加深度能提高准确性，能不能*至少让深度网络实现和浅层网络一样的性能，即让深度网络后面的层至少实现恒等映射的作用*，根据这个想法，作者提出了residual模块来帮助网络实现恒等映射。
### 3.2 Residual Module
![img](https://img-blog.csdnimg.cn/201812171934125.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzYyNDUzOA==,size_16,color_FFFFFF,t_70)  
- 根据上图，copy一个浅层网络的输出加给深层的输出，这样当网络特征达到optimal的时候更深层恒等映射的任务就从原来堆叠的层中释放到新建的这个恒等映射关系中，而原来层中的任务就从恒等映射转为全0
- F(x)=H(x)−x，x为浅层的输出，H(x)为深层的输出,F(x)为夹在二者中间的的两层代表的变换，当浅层的x代表的特征已经足够成熟，如果任何对于特征x的改变都会让loss变大的话，F(x)会自动趋向于学习成为0，x则从恒等映射的路径继续传递。这样就在不增加计算成本的情况下实现了一开始的目的：**在前向过程中，当浅层的输出已经足够成熟（optimal），让深层网络后面的层能够实现恒等映射的作用**  
***  
![img](https://img-blog.csdnimg.cn/20181217193149794.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzYyNDUzOA==,size_16,color_FFFFFF,t_70)  
从另一个角度看，在反向传播中，residual模块会起到什么样的作用呢  
- residual模块将输出分成F(x)+x两部分，其中F依然是x的函数，也就是说F实际上是对于x的补充，是对于x的fun-tuning，这样就把任务从根据x映射成一个新的y转为了根据x求x和y之间的差距，这明显是一个相对更加简单的任务，论文是这么写的，到底怎么简单的，我们来分析一下。  
- 举个例子，假设不加residual模块的输出为h(x)。x=10,h(x)=11。h简化为线性运算Wh。Wh明显为1.1，加了redidual模块后，F(x)=1，H(x)=F(x)+x=11，F也简化为线性运算,对应的WF为0.1。当标签中的真实值为12，反向传播的损失为1，而对于F中的参数和h中参数回传的损失实际上是一样大的而且梯度都是x的值，但是对于F的参数就从0.1到0.2，而h的参数是从1.1到1.2，**因此redidual模块会明显减小模块中参数的值从而让网络中的参数对反向传导的损失值有更敏感的响应能力，虽然根本上没有解决回传的损失小得问题，但是却让参数减小，相对而言增加了回传损失的效果，也产生了一定的正则化作用。**  
- 其次，因为前向过程中有恒等映射的支路存在，因此在反向传播过程中梯度的传导也多了更简便的路径，仅仅经过一个relu就可以把梯度传达给上一个模块。
- 所谓反向传播就是网络输出一个值，然后与真实值做比较的到一个误差损失，同时将这个损失做差改变参数，返回的损失大小取决于原来的损失和梯度，既然目的是为了改变参数，而问题是改变参数的力度过小，则可以减小参数的值，使损失对参数改变的力度相对更大。  
- 因此残差模块最重要的作用就是改变了前向和后向信息传递的方式从而很大程度上促进了网络的优化。  
***  
![img](https://img-blog.csdnimg.cn/2018121719325477.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzYyNDUzOA==,size_16,color_FFFFFF,t_70)   
- 利用Inceptionv3提出的四个准则我们再用一下以改进residual模块，利用准则3，再空间聚合之前先进行降维不会发生信息丢失，所以这里也采用了同样的方法，加入1×1的卷积核用来增加非线性和减小输出的深度以减小计算成本。就得到了成为bottleneck的residual模块形式。上图左为basic形式，右为bottleneck的形式。(1×1的卷积是降维的作用)  
- 综上所述，shortcut模块会在前向过程中帮助网络中的特征进行恒等映射，在反向过程中帮助传导梯度，让更深的模型能够成功训练。  
***  
![img](https://img-blog.csdnimg.cn/20181217193313388.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzYyNDUzOA==,size_16,color_FFFFFF,t_70)  
左边为基础的VGG，中间为基于VGG作出的扩增至34层的普通网络，右边为34层的残差网络，不同的是每隔两层就会有一个residual模块。  

## 4 Experientment
### Experientment about ResNet-100
以往模型大多在ImageNet上作测试，所以这里只给出在ImageNet上的成绩，论文还在CIFAR-10/100上做了测试。  
ResNet具体代码见：[ResNet](https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py)  
![img](https://img-blog.csdnimg.cn/20181217193335918.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzYyNDUzOA==,size_16,color_FFFFFF,t_70)  
网络配置:需要注意的一点是对于block和block之间的特征大小不同时，因为是在F内部发生变化，x也需要随之变化才能对应相加，论文中采用的调整方法是采用0填充来拟补深度，但是如何缩小特征尺寸没有说，而且官网的复现方式都是直接用一个1×1的卷积核来操作这一步，理论上讲会破坏恒等映射。  
Result:  
![img](https://img-blog.csdnimg.cn/20181217193440759.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzYyNDUzOA==,size_16,color_FFFFFF,t_70)  
152层的ResNet相比于其他网络有提高了一些精度，并且ResNet的参数量为1.1千万，VGG16参数数量为1.53千万，可见虽然ResNet深度增加了近十倍，但是参数量因为使用bottleneck模块反而更少  

### From 100 to 1000 layers
- 接着上面的ResNet想，可以说ResNet的成功是借助于residual模块的引入很好的解决了网络degradation的问题，从而提高了网络深度，得到了表达能力更强的特征，有了更高的准确度。那么我们还能不能加深一些呢？100层可以，1000层呢？  
- 答案是不可以，至少目前的残差模型是不行的，因为目前的残差块在加和之后会经过一个relu，这增加的操作虽然在100层中不会有很大的影响，但是在1000层的超深网络里面还是会阻碍整个网络的前向反响传播，我们需要接着改进。

### Improve residual module  
$y_l=h(x_l)+F(x_l,W_l)$ (1)  $x_{l+1}=f(y_l)$ (2)  
- 上式是残差模块中的基本形式，h(x)是恒等映射，F是网络中的变化，f(x)是对于叠加之后值的变换，在原始残差模块中是relu，网络通过学习其中的F的参数来减小loss值。我们希望能够为整个网络信息的传播构造一个流畅的通道，这就要求h和f都必须是恒等映射,即：
  - 如果f也是恒等映射的话，那么$x_{l+1}≡y$,我们就可以把公式(2)并入到公式(1)，得到:$x_{l+1}=x_l+F(x_l,W_l)$  
  - 其中$x_l$又可以拆分为上一模块的输出和l层残差模块的加和，因此循环递归得到以下公式：  
  - ![img](https://github.com/Kittyuzu1207/Share/blob/master/img/04181.png)  
  - 可以看到，对于L层的输出而言，可以看作任何一个L层之前的l层的输出x_l和中间残差块的输出的叠加（注意中间残差块的输入也是随着i变化的，因此每个残差块内部也都有l层输出x_l的作用），因此整个网络是residual fashion的，可以把任何一层和该层之前的任何一层看成残差模块，这样就保证了整个网络的前向传播的畅通。  
- 改进后网络的反向传播公式如下：  
![img](https://github.com/Kittyuzu1207/Share/blob/master/img/04182.png)  
- 可以看到，对于任何一层的x的梯度由两部分组成，其中一部分直接就由L层不加任何衰减和改变的直接传导l层，这保证了梯度传播的有效性,另一部分也由链式法则的累乘变为了累加，这样有更好的稳定性。  

### The importance of identity mapping
我们从另外一个角度来看identity mapping在h和f中的重要性，首先来分析h，如果h不是恒等映射，而是网络中一些其他很常见的操作如卷积，我们简化卷积操作为简单的给h乘上一个系数λ,这样原来的前向传播公式(4)就变成了:  
![img](https://github.com/Kittyuzu1207/Share/blob/master/img/04183.png)  
当λ大于1，传播回来的梯度因为累乘会变得很大，会产生梯度爆炸的效应，让网络更难训练，当λ小于1，从shortcut传播回的梯度会趋近于0，这样网络就要回到最初的起点从参数层传播，我们知道对于很深的网络而言这种方式是很难传播的。因此对于h设为恒等映射对于深度网络的训练最有效。  

### 残差模块的几种可能的设置
![img](https://img-blog.csdnimg.cn/20181217193522106.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzYyNDUzOA==,size_16,color_FFFFFF,t_70)  
- 然后我们来分析f为恒等映射的重要性和相关设置。在原始残差模块中，F和h相加之后经过一个relu再输到下一个block，我们有那么几种做法来去掉relu操作来保持f的identity mapping，第一个就是把relu放回参数层F中，如上图中的c，但这样会让F中拟合的对于x的残差只有正值，会大大减小残差的表示性  
- 文章提出了一种叫pre-activation的方式，即把BN和relu放在卷积的前面，这样就可以保证F中所有的操作都在和x相加之前完成，并且不会对残差产生限制，上图中的(e)。实际上把激活层（relu+BN）放在卷积的前面的操作在VGG等网络中不会产生不同的影响，但是在残差网络中就可以保证输入和输出加和之后在输入下一层之前没有别的操作，让整个信息的前向后向流动没有任何阻碍，从而让模型的优化更加简单和方便。  
- 对于(d)这种只把relu提前的操作也会产生问题，当F中经过最后一个BN后，还要经过一个和x相加的运算，本来BN就是为了给数据一个固定的分布，一旦经过别的操作就会改变数据的分布，会削减BN的作用。在原版本的resnet中就是这么使用的BN，所以这种pre-activation的方式也增加了残差模块的正则化作用

### Experiment about ResNet-1000
**100层的不同残差网络比较**  
![img](https://img-blog.csdnimg.cn/20181217193551381.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzYyNDUzOA==,size_16,color_FFFFFF,t_70)  
和原始152层残差模型相比，改进后的残差模型在精度上并没有很大的提升，这说明原始的残差模块在152层的网络上已经有足够好的性能。（这也是为什么152层的模型代码大多依然保持最开始的残差模块）  
***  
**1000层不同残差网络比较**
![img](https://img-blog.csdnimg.cn/20181217193613854.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzYyNDUzOA==,size_16,color_FFFFFF,t_70)  
相比于原始1000层残差模型，改进残差块后有了很大的精度提升，究其原因是改进后的残差模块提供了更加有效的反向传播方式，让梯度更有效的传导到模型任何一层，让训练更加容易，从而激发了更多深度模型的性能。  
![img](https://img-blog.csdnimg.cn/20181217194252818.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzYyNDUzOA==,size_16,color_FFFFFF,t_70)  

## other residual module  
### More wild residual 增加宽度  
残差网络主要是研究深度对于网络的性能影响，并且在增加深度的同时为了减少计算量尽量让模型变瘦，用了bottleneck模块等技巧让每层的卷积核数量尽量少，但模型的深度到达一定程度之后，参数尽管大幅上升，但是不能带来相对应的性能的大幅提升，于是[《Wide Residual Networks》](https://arxiv.org/abs/1605.07146)这篇论文分析研究了宽度（每层卷积核数量）对于残差网络的影响，并用16层的改进残差网络就达到了1000层残差网络的性能，尽管提高了参数的数量，却需要更短的训练时间。  
结论：增大宽度可以增加各种深度的残差模型的性能/只要参数的数量可以接受，宽度和深度的增加就可以使性能提升/在相同的参数数量下，更深的模型并不比更宽的模型有更好的性能  
### Inception v4
GoogLeNet的Inception模块是很好的增加单层特征表达性能的方式，因此Inception和Residual二者就很好的结合了，具体细节在[《Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning》](https://www.researchgate.net/publication/301874967_Inception-v4_Inception-ResNet_and_the_Impact_of_Residual_Connections_on_Learning)中。主要思想为利用残差模块来帮助提升深度和加速训练，同时保持Inception的多分支卷积的形式来帮助增加前向过程的特征表达能力  
### ResneXt
wide residual研究了宽度对于残差网络的重要性，并提出了网络性能提升的关键在于残差模块，而不是shortcut，ResNeXt基于wide residual和inception，提出了另一个方向，即将残差模块中的所有通道分组进行汇合操作会有更好的效果，同时也给inception提出了一个抽象化的表示方式。  

参考博客(https://blog.csdn.net/weixin_43624538/article/details/85049699)
