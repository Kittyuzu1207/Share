# ResNet残差网络理解  
## 1 Background 提出背景  
### Forward：from shallow to deep
- 深度学习省去了人工寻找特征的步骤，不同的模型也找出不同质量的特征，特征的质量直接影响到分类结果的准确度，表达能力更强的特征也给模型带来更强的分类能力。
- 特征也可以根据复杂度和表示能力粗略的分为高中低三种种类，理论上讲越复杂的特征有越强的表征能力。一般越深的网络输出表示能力越强的特征。所以，网络的深度对于学习表达能力更强的特征至关重要。
- 深度模型中，每层的输出特征图的尺寸大都随着网络深度而变化，主要是高和宽越来越小，输出特征图的深度随着网络层数的深度而增加。高和宽的减小有助于减小计算量，而特征图深度的增加则使每层输出中可用特征数量的增多。

### Backward：the problem caused by increasing depth
- 增加深度带来的首个问题就是梯度爆炸/消散的问题，这是由于随着层数的增多，在网络中反向传播的梯度会随着连乘变得不稳定，变得特别大或者特别小。
- 为了克服梯度消散也想出了许多的解决办法，如使用BatchNorm，将激活函数换为ReLu，使用Xaiver初始化等，可以说梯度消散已经得到了很好的解决。
- 增加深度的另一个问题就是网络的degradation问题，即随着深度的增加，网络的性能会越来越差，直接体现为在训练集上的准确率会下降。**ResNet就是解决这个问题**  

## 2 Degradation of deep network
