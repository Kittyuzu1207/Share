# 20200526初识GCN
参考资料：[GCN总结](https://www.cnblogs.com/nxf-rabbit75/p/11306198.html#auto-id-0)  
图的学习任务：
- 图节点分类任务
- 图变结构预测任务
- 图的分类  

GNN模型主要研究图节点的表示（Graph Embedding），这些问题都是基于这个展开的。  
- spatial domain空间域：DeepWalk，LINE,Node2Vec，Struc2Vec...
- spectral domain 谱域：第1-4代GCN，Multi-Graph   

谱卷积有理论支持，但有时候会受到拉普拉斯算子的限制；而空间域卷积更加灵活，主要困难在于选择定量邻域上，没有统一理论。
未来方向：加深网络/感受野/可扩展性/动态性和异质性  
GCN：  
- 对卷积神经网络在 graph domain 上的自然推广
- 它能同时对节点特征信息与结构信息进行端对端学习，是目前对图数据学习任务的最佳选择。
- 图卷积适用性极广，适用于任意拓扑结构的节点与图。
- 在节点分类与边预测等任务上，在公开数据集上效果要远远优于其他方法。

## spectral  domain
### 1.离散卷积  
离散卷积本质：加权求和CNN中的卷积本质上就是利用一个共享参数的过滤器（kernel），**通过计算中心像素点以及相邻像素点的加权和来构成feature map实现空间特征的提取**，当然加权系数就是卷积核的权重系数。   
卷积核的系数如何确定？是随机化初值，然后根据误差函数通过反向传播梯度下降进行迭代优化。**卷积核的参数通过优化求出才能实现特征提取的作用，GCN的理论很大一部分工作就是为了引入可以优化的卷积参数。**
### 2.GCN中的Graph
**CNN处理的图像或者视频数据中像素点（pixel）是排列成成很整齐的矩阵** Euclidean Structure，比较方方正正这种...  
科学研究中还有很多Non Euclidean Structure的数据，如，社交网络、信息网络中有很多类似的结构。这样的网络结构（Non Euclidean Structure）就是图论中抽象意义上的拓扑图。  
**Graph Convolutional Network中的Graph是指数学（图论）中的用顶点和边建立相应关系的拓扑图。**  
为什么要研究GCN？  
1.因为CNN无法处理非欧几里得结构的数据，学术上的表述是传统的离散卷积在非欧几里得结构的数据上无法保持平移不变性。通俗来说就是在拓扑图中每个顶点的相邻顶点数目都可能不同，那就没办法用一个同样尺寸的卷积核来进行卷积操作  
2.广义来讲任何数据在赋范空间内都可以建立拓扑关联，谱聚类就是应用了这样的思想，所以说拓扑连接是一种广义的数据结构，GCN有很大的应用空间。  

## 3.提供拓扑图空间特征的两种方式
### 1. vertex domain（spatial domain）  
提取拓扑图上的空间特征，就是把每个顶点的相邻neighbors找出来.  
按照什么条件去找中心vertex的neighbors，如何确定receptive filed?确定了receptive field之后，按照什么方式处理包含不同数目neighbors的特征？  
Learning Convolutional Neural Networks for Graphs  
缺点：每个顶点提取出来的neighbors不同，使得计算处理必须针对每个顶点；提取特征的效果可能没有卷积好  
### 2.spectral domain
Q1 什么是Spectral graph theory？  
简单的概括就是**借助于图的拉普拉斯矩阵的特征值和特征向量来研究图的性质**  

## 4.什么是拉普拉斯矩阵？为什么GCN要用拉普拉斯矩阵？
对于图G=(V,E)其Laplacian 矩阵的定义为 L=D−A，其中L是Laplacian 矩阵，D 是顶点的度矩阵（对角矩阵）,对角线上元素依次为各个顶点的度，A是图的邻接矩阵。  
常用的拉普拉斯矩阵实际用三种： 
NO.1：Combinatorial Laplacian：L=D−AL=D−A  
NO.2：Symmetric normalized Laplacian：Lsys=D−1/2LD−1/2Lsys=D−1/2LD−1/2  
NO.3：Random walk normalized Laplacian：Lrw=D−1L  
为什么GCN要用拉普拉斯矩阵？  
(1)拉普拉斯矩阵是对称矩阵，可以进行特征分解（谱分解），这就和GCN的spectral domain对应上了  
(2)拉普拉斯矩阵只在中心顶点和一阶相连的顶点上（1-hop neighbor）有非0元素，其余之处均为0  
(3)通过拉普拉斯算子与拉普拉斯矩阵进类比  

## 5.拉普拉斯矩阵的谱分解（特征分解）
矩阵的谱分解，特征分解，对角化都是同一个概念。**不是所有的矩阵都可以特征分解**，其充要条件为n阶方阵存在n个线性无关的特征向量。
**但是拉普拉斯矩阵是半正定对称矩阵**（半正定矩阵本身就是对称矩阵）有如下三个性质：  
- 对称矩阵一定n个线性无关的特征向量
- 半正定矩阵的特征值一定非负
- 对阵矩阵的特征向量相互正交，即所有特征向量构成的矩阵为正交矩阵
由上可以知道拉普拉斯矩阵一定可以谱分解，且分解后有特殊的形式。  

## 6.如何从传统的傅里叶变换、卷积类比到Graph上的傅里叶变换及卷积？
把传统的傅里叶变换以及卷积迁移到Graph上来，核心工作其实就是把拉普拉斯算子的特征函数e^−iwt变为Graph对应的拉普拉斯矩阵的特征向量。  
### （1）推广傅里叶变换
#### (a)Graph上的傅里叶变换
传统的傅里叶变换：F(W)=F([t])=∫f(t)e^-iwtdt。 即信号f(t)与基函数e−iwt的积分。为什么要找e−iwt作为基函数呢？e−iwt是拉普拉斯算子的特征函数，w就和特征值有关。  
LV=λV，L是拉普拉斯矩阵，V是其特征向量。离散积分就是一种内积形式，仿上定义Graph上的傅里叶变换。  
f_hat=U^Tf
#### (b)Graph上的傅里叶逆变换
f=Uf_hat  
### （2）推广卷积
卷积定理：函数卷积的傅里叶变换是函数傅立叶变换的乘积，即对于函数f(t)f(t)与h(t)h(t)两者的卷积是其函数傅立叶变换乘积的逆变换：

## 7.为什么拉普拉斯矩阵的特征向量可以作为傅里叶变换的基？特征值表示频率？
### （1）为什么拉普拉斯矩阵的特征向量可以作为傅里叶变换的基？
傅里叶变换一个本质理解就是：**把任意一个函数表示成了若干个正交函数（由sin,cos 构成）的线性组合。** graph傅里叶变换也把graph上定义的任意向量 ![公式](https://www.zhihu.com/equation?tex=f) ，表示成了拉普拉斯矩阵特征向量的线性组合.那么：为什么graph上任意的向量f都可以表示成这样的线性组合？原因在于(u1→,u2→,...,un−→)(u1→,u2→,...,un→)是graph上n维空间中的n个线性无关的正交向量.  
（2）怎么理解拉普拉斯矩阵的特征值表示频率？  
在graph空间上无法可视化频率的概念，信息论告诉我们，特征值越大，对应的信息越多，小的特征值就是低频分量，信息较少，是可以忽略的。
在压缩图像的过程中，也是把低频成分变为0，高频（边缘）会被保留，它带给我们更多的信息.  

## 8.深度学习中的图卷积
### （1）第一代GCN  
来自论文：[Spectral Networks and Locally Connected Networks on Graphs](https://arxiv.org/abs/1312.6203)
把diag(h^(λl))变成了卷积核diag(θl)  
y_output=σ(Ugθ(Λ)UTx)  
(为避免混淆，下面称gθ(Λ)gθ(Λ)是卷积核，Ugθ(Λ)UUgθ(Λ)U的运算结果为卷积运算矩阵)  
Θ=(θ1,θ2,...,θn)就跟三层神经网络中的weight一样是任意的参数，通过初始化赋值然后利用误差反向传播进行调整，xx就是graph上对应于每个顶点的feature vector（由数据集提取特征构成的向量）  
弊端： 
- 每一次前向传播，都要计算U、gθ(Λ)、UTU、gθ(Λ)、UT三者的矩阵乘积，特别是对于大规模的graph，计算的代价较高，复杂度为O(n2)O(n2)
- 卷积核不具有spatial localization
- 卷积核需要n个参数
### （2）第二代GCN
来自论文：[Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering](https://arxiv.org/abs/1606.09375)  
y_output就变成了y_output=σ(∑Kj=0αjLjx)
其中(α1,α2,⋯,αK)(α1,α2,⋯,αK)是任意的参数，通过初始化赋值然后利用误差反向传播进行调整。  
**优点**：
- 卷积核只有K个参数，一般K远小于n，参数的复杂度被大大降低了；
- 矩阵变换后，不需要做特征分解了，直接用拉普拉斯矩阵L进行变换，然而由于要计算Lj，计算复杂度还是O(n2)
- 卷积核具有很好的spatial localization，特别的，K就是卷积核的receptive field，也就是说每次卷积会将中心节点K-hop neighbor上的feature进行加权求和，权系数就是αk。更直观地看，K=1K=1就是对每个顶点上一阶neighbor的feature进行加权求和。
**弊端**:参数太少了，只有KK个，使得模型无法很好地实现在同阶的邻域上分配不同的权重给不同的邻居（也就是GAT论文里说的 enable specifying different weights to different nodes in a neighborhood）

### （3）第三代GCN---降低计算复杂度
来自论文：[Wavelets on graphs via spectral graph theory](https://hal.inria.fr/inria-00541855)  
在第二代GCN中，LL是n* n的矩阵，所以LjLj的计算还是O(n2)O(n2)的，《Wavelets on graphs via spectral graph theory》提出了利用Chebyshev多项式拟合卷积核的方法，来逼近计算复杂度。卷积核gθ(Λ)可以用截断的(truncated)的shifted Chebyshev多项式来逼近。  

### （4）第四代GCN
来自论文：[Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907)  
进一步对卷积核的计算做了近似，产生了我们所熟知的GCN逐层传播公式。 在GCN模型中，作者首先做了额外的两个假设：λmax≈2,K=1 这两个假设可以大大简化模型。而作者希望假设造成的误差可以通过神经网络参数训练过程来自动适应。
最后得到了大家耳熟能详的GCN逐层更新公式：输入节点矩阵X∈RN×CX∈RN×C, 每个输入节点有C个通道(channels, 即每个图节点有C维特征)，卷积操作包含F个滤波器(filters)或特征映射(feature maps).

## 9.关于有向图问题
如果是有向图问题，最大的区别就是邻接矩阵会变成非对称矩阵，这个时候不能直接定义拉普拉斯矩阵及其谱分解，有两条思路解决问题:
（1）要想保持理论上的完美，就需要重新定义图的邻接关系，保持对称性。比如《MotifNet: a motif-based Graph Convolutional Network for directed graphs》提出利用Graph Motifs定义图的邻接关系。  
（2）如果只是为了应用，有其他形式的GCN或者GAT可以处理有向图  


## 经典算法
