# 20200526初识GCN
参考资料：[GCN总结](https://www.cnblogs.com/nxf-rabbit75/p/11306198.html#auto-id-0)  
图的学习任务：
- 图节点分类任务
- 图变结构预测任务
- 图的分类  

GNN模型主要研究图节点的表示（Graph Embedding），这些问题都是基于这个展开的。  
- spatial domain空间域：DeepWalk，LINE,Node2Vec，Struc2Vec...
- spectral domain 谱域：第1-4代GCN，Multi-Graph   

谱卷积有理论支持，但有时候会受到拉普拉斯算子的限制；而空间域卷积更加灵活，主要困难在于选择定量邻域上，没有统一理论。
未来方向：加深网络/感受野/可扩展性/动态性和异质性  
GCN：  
- 对卷积神经网络在 graph domain 上的自然推广
- 它能同时对节点特征信息与结构信息进行端对端学习，是目前对图数据学习任务的最佳选择。
- 图卷积适用性极广，适用于任意拓扑结构的节点与图。
- 在节点分类与边预测等任务上，在公开数据集上效果要远远优于其他方法。

## spectral  domain
### 1.离散卷积  
离散卷积本质：加权求和CNN中的卷积本质上就是利用一个共享参数的过滤器（kernel），**通过计算中心像素点以及相邻像素点的加权和来构成feature map实现空间特征的提取**，当然加权系数就是卷积核的权重系数。   
卷积核的系数如何确定？是随机化初值，然后根据误差函数通过反向传播梯度下降进行迭代优化。**卷积核的参数通过优化求出才能实现特征提取的作用，GCN的理论很大一部分工作就是为了引入可以优化的卷积参数。**
### 2.GCN中的Graph
**CNN处理的图像或者视频数据中像素点（pixel）是排列成成很整齐的矩阵** Euclidean Structure，比较方方正正这种...  
科学研究中还有很多Non Euclidean Structure的数据，如，社交网络、信息网络中有很多类似的结构。这样的网络结构（Non Euclidean Structure）就是图论中抽象意义上的拓扑图。  
**Graph Convolutional Network中的Graph是指数学（图论）中的用顶点和边建立相应关系的拓扑图。**  
为什么要研究GCN？  
1.因为CNN无法处理非欧几里得结构的数据，学术上的表述是传统的离散卷积在非欧几里得结构的数据上无法保持平移不变性。通俗来说就是在拓扑图中每个顶点的相邻顶点数目都可能不同，那就没办法用一个同样尺寸的卷积核来进行卷积操作  
2.广义来讲任何数据在赋范空间内都可以建立拓扑关联，谱聚类就是应用了这样的思想，所以说拓扑连接是一种广义的数据结构，GCN有很大的应用空间。  

## 3.提供拓扑图空间特征的两种方式
### 1. vertex domain（spatial domain）  
提取拓扑图上的空间特征，就是把每个顶点的相邻neighbors找出来.  
按照什么条件去找中心vertex的neighbors，如何确定receptive filed?确定了receptive field之后，按照什么方式处理包含不同数目neighbors的特征？  
Learning Convolutional Neural Networks for Graphs  
缺点：每个顶点提取出来的neighbors不同，使得计算处理必须针对每个顶点；提取特征的效果可能没有卷积好  
### 2.spectral domain
Q1 什么是Spectral graph theory？  
简单的概括就是**借助于图的拉普拉斯矩阵的特征值和特征向量来研究图的性质**  

## 4.什么是拉普拉斯矩阵？为什么GCN要用拉普拉斯矩阵？
对于图G=(V,E)其Laplacian 矩阵的定义为 L=D−A，其中L是Laplacian 矩阵，D 是顶点的度矩阵（对角矩阵）,对角线上元素依次为各个顶点的度，A是图的邻接矩阵。  
常用的拉普拉斯矩阵实际用三种： 
NO.1：Combinatorial Laplacian：L=D−AL=D−A  
NO.2：Symmetric normalized Laplacian：Lsys=D−1/2LD−1/2Lsys=D−1/2LD−1/2  
NO.3：Random walk normalized Laplacian：Lrw=D−1L  
为什么GCN要用拉普拉斯矩阵？  
(1)拉普拉斯矩阵是对称矩阵，可以进行特征分解（谱分解），这就和GCN的spectral domain对应上了  
(2)拉普拉斯矩阵只在中心顶点和一阶相连的顶点上（1-hop neighbor）有非0元素，其余之处均为0  
(3)通过拉普拉斯算子与拉普拉斯矩阵进类比  

## 5.拉普拉斯矩阵的谱分解（特征分解）
矩阵的谱分解，特征分解，对角化都是同一个概念。**不是所有的矩阵都可以特征分解**，其充要条件为n阶方阵存在n个线性无关的特征向量。
**但是拉普拉斯矩阵是半正定对称矩阵**（半正定矩阵本身就是对称矩阵）有如下三个性质：  
- 对称矩阵一定n个线性无关的特征向量
- 半正定矩阵的特征值一定非负
- 对阵矩阵的特征向量相互正交，即所有特征向量构成的矩阵为正交矩阵
由上可以知道拉普拉斯矩阵一定可以谱分解，且分解后有特殊的形式。  

## 6.如何从传统的傅里叶变换、卷积类比到Graph上的傅里叶变换及卷积？
把传统的傅里叶变换以及卷积迁移到Graph上来，核心工作其实就是把拉普拉斯算子的特征函数e^−iwt变为Graph对应的拉普拉斯矩阵的特征向量。  
### （1）推广傅里叶变换
#### (a)Graph上的傅里叶变换
传统的傅里叶变换：F(W)=F([t])=∫f(t)e^-iwtdt。 即信号f(t)与基函数e−iwt的积分。为什么要找e−iwt作为基函数呢？e−iwt是拉普拉斯算子的特征函数，w就和特征值有关。  
LV=λV，L是拉普拉斯矩阵，V是其特征向量。离散积分就是一种内积形式，仿上定义Graph上的傅里叶变换。  
f_hat=U^Tf
#### (b)Graph上的傅里叶逆变换
f=Uf_hat  
### （2）推广卷积
卷积定理：函数卷积的傅里叶变换是函数傅立叶变换的乘积，即对于函数f(t)f(t)与h(t)h(t)两者的卷积是其函数傅立叶变换乘积的逆变换：

## 7.为什么拉普拉斯矩阵的特征向量可以作为傅里叶变换的基？特征值表示频率？
### （1）为什么拉普拉斯矩阵的特征向量可以作为傅里叶变换的基？
傅里叶变换一个本质理解就是：**把任意一个函数表示成了若干个正交函数（由sin,cos 构成）的线性组合。** graph傅里叶变换也把graph上定义的任意向量 ![公式](https://www.zhihu.com/equation?tex=f) ，表示成了拉普拉斯矩阵特征向量的线性组合.那么：为什么graph上任意的向量f都可以表示成这样的线性组合？原因在于(u1→,u2→,...,un−→)(u1→,u2→,...,un→)是graph上n维空间中的n个线性无关的正交向量.  
（2）怎么理解拉普拉斯矩阵的特征值表示频率？  
在graph空间上无法可视化频率的概念，信息论告诉我们，特征值越大，对应的信息越多，小的特征值就是低频分量，信息较少，是可以忽略的。
在压缩图像的过程中，也是把低频成分变为0，高频（边缘）会被保留，它带给我们更多的信息.  

## 8.深度学习中的图卷积
### （1）第一代GCN  
来自论文：[Spectral Networks and Locally Connected Networks on Graphs](https://arxiv.org/abs/1312.6203)
把diag(h^(λl))变成了卷积核diag(θl)  
y_output=σ(Ugθ(Λ)UTx)  
(为避免混淆，下面称gθ(Λ)gθ(Λ)是卷积核，Ugθ(Λ)UUgθ(Λ)U的运算结果为卷积运算矩阵)  
Θ=(θ1,θ2,...,θn)就跟三层神经网络中的weight一样是任意的参数，通过初始化赋值然后利用误差反向传播进行调整，xx就是graph上对应于每个顶点的feature vector（由数据集提取特征构成的向量）  
弊端： 
- 每一次前向传播，都要计算U、gθ(Λ)、UTU、gθ(Λ)、UT三者的矩阵乘积，特别是对于大规模的graph，计算的代价较高，复杂度为O(n2)O(n2)
- 卷积核不具有spatial localization
- 卷积核需要n个参数
### （2）第二代GCN
来自论文：[Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering](https://arxiv.org/abs/1606.09375)  
y_output就变成了y_output=σ(∑Kj=0αjLjx)
其中(α1,α2,⋯,αK)(α1,α2,⋯,αK)是任意的参数，通过初始化赋值然后利用误差反向传播进行调整。  
**优点**：
- 卷积核只有K个参数，一般K远小于n，参数的复杂度被大大降低了；
- 矩阵变换后，不需要做特征分解了，直接用拉普拉斯矩阵L进行变换，然而由于要计算Lj，计算复杂度还是O(n2)
- 卷积核具有很好的spatial localization，特别的，K就是卷积核的receptive field，也就是说每次卷积会将中心节点K-hop neighbor上的feature进行加权求和，权系数就是αk。更直观地看，K=1K=1就是对每个顶点上一阶neighbor的feature进行加权求和。
**弊端**:参数太少了，只有KK个，使得模型无法很好地实现在同阶的邻域上分配不同的权重给不同的邻居（也就是GAT论文里说的 enable specifying different weights to different nodes in a neighborhood）

### （3）第三代GCN---降低计算复杂度
来自论文：[Wavelets on graphs via spectral graph theory](https://hal.inria.fr/inria-00541855)  
在第二代GCN中，LL是n* n的矩阵，所以LjLj的计算还是O(n2)O(n2)的，《Wavelets on graphs via spectral graph theory》提出了利用Chebyshev多项式拟合卷积核的方法，来逼近计算复杂度。卷积核gθ(Λ)可以用截断的(truncated)的shifted Chebyshev多项式来逼近。  

### （4）第四代GCN
来自论文：[Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907)  
进一步对卷积核的计算做了近似，产生了我们所熟知的GCN逐层传播公式。 在GCN模型中，作者首先做了额外的两个假设：λmax≈2,K=1 这两个假设可以大大简化模型。而作者希望假设造成的误差可以通过神经网络参数训练过程来自动适应。
最后得到了大家耳熟能详的GCN逐层更新公式：输入节点矩阵X∈RN×CX∈RN×C, 每个输入节点有C个通道(channels, 即每个图节点有C维特征)，卷积操作包含F个滤波器(filters)或特征映射(feature maps).

## 9.关于有向图问题
如果是有向图问题，最大的区别就是邻接矩阵会变成非对称矩阵，这个时候不能直接定义拉普拉斯矩阵及其谱分解，有两条思路解决问题:
（1）要想保持理论上的完美，就需要重新定义图的邻接关系，保持对称性。比如《MotifNet: a motif-based Graph Convolutional Network for directed graphs》提出利用Graph Motifs定义图的邻接关系。  
（2）如果只是为了应用，有其他形式的GCN或者GAT可以处理有向图  


## 经典算法
### （一）Spectral domain
#### 1.半监督学习节点分类---两层GCN
论文：“SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS“,2017 ICLR  
总结：如果数据可以构成图，可以考虑下图卷积GCN，将卷积网络用于图数据上能对网络中的节点或者整个图进行分类，能利用节点的属性和节点的label进行训练，但这个算法不适应与规模大的图结构，因为GCN需要输入整个邻接矩阵A和特征矩阵X，这是非常耗内存的。  
#### 2.基于多图卷积网络的自行车流量预测
问题描述：利用t时刻以前的进站流和出站流预测t时刻的进站流和出站流
三个模块：
构建和融合多图：定义站点间不同关系并融合得到Graph。
GCN+encoder-decoder模型：生成包含时间、空间、流量等信息的embeddingMLP：
结合embedding和节假日、天气等其他特征去预测站点流量。  
本文构建了多张空间关系，通过多张图得到预测效果。encoder-decoder的结构存在局限性，输入输出的维度是固定的。当预测短期的流量时，本文的模型更适用。

### （二）Spatial domain
#### 1.DeepWalk( Random walk + Word2vec)
类似skip-gram模型  
Transductive Learning  
Corpus : Graph作为”语料库”;随机游走(采样)构建”句子”  
可并行进行多个随机游走  
DeepWalk的主要问题是它缺乏泛化能力。 每当有新节点加入到图中时，它必须重新训练模型以正确表示该节点（ 直推式学习 ）。 因此，这种GNN不适用于图中节点不断变化的动态图  

#### 2.GraphSAGE( Graph SAmple and aggreGatE)
Inductive Learning  
Deepwalk,LINE, node2vec, SDNE等模型能够高效地得到每个节点的embedding。然而，这些方法无法有效适应动态图中新增节点的特性， 往往需要从头训练或至少局部重训练。  
通过对节点的本地邻域中的特征进行采样和聚合（平均/LSTM/pooling聚合）来生成嵌入，而不是为每个节点训练单个嵌入。  
聚合器参数和权重变量的学习：有监督情况下，可以使用每个节点的预测label和真实label的交叉熵作为损失函数；无监督情况下，可以假设相邻节点的输出embedding应当尽可能相近，保证相邻节点的embedding的相似度尽量大的情况下，保证不相邻节点的embedding的期望相似度尽可能小。  

#### 3.GAT-加入注意力
基本的图神经网络算法GCN, 使用采样和聚合构建的inductive learning框架GraphSAGE, 然而图结构数据常常含有噪声，意味着节点与节点之间的边有时不是那么可靠，邻居的相对重要性也有差异，解决这个问题的方式是在图算法中引入“注意力”机制(attention mechanism), 通过计算当前节点与邻居的“注意力系数”(attention coefficient), 在聚合邻居embedding的时候进行加权，使得图神经网络能够更加关注重要的节点，以减少边噪声带来的影响。  
- 学习注意力权重（Learn attention weights）
- 相似性注意力（Similarity-based attention）
- 注意力引导的随机游走（Attention-guided walk）

## examples
- 节点分类—反欺诈：因为图中每个节点都拥有自己的特征信息。通过该特征信息，我们可以构建一个风控系统，如果交易节点所关联的用户 IP 和收货地址与用户注册 IP 和注册地址不匹配，那么系统将有可能认为该用户存在欺诈风险。
- 边结构预测—商品推荐：图中每个节点都具有结构信息。如果用户频繁购买某种类别商品或对某种类别商品评分较高，那么系统就可以认定该用户对该类商品比较感兴趣，所以就可以向该用户推荐更多该类别的商品。
